---
layout: post
title:  "微服务运维实践-平滑上线"
date:   2016-05-24 20:22:22 +0800
categories: microservice
---

## 1. 摘要

SSP 团队在部门的技术规划下， 逐步采用了微服务架构， 拆分先前的单体应用。
在微服务的实践过程中， `1. 遇到了[上线复杂度增加]的问题。 2. 并且， JPaaS平台提供
的灰度上线方案只支持单体应用的上线， 并不能够支持服务化, ［上线过程中， 服务有不可用时期]。` 
在这样的背景下， 我们对上线流程进行改进， 升级了注册发现和服务调用的逻辑。 开发了上线程序复用
Go-BFE 和Disconf 做到了`1. [一键上线, 一键回滚], 2. 拆分上线步骤， [上线程序的时候用户不感知]。 在上线
完成并验证通过后通过切换流量完成上线。3. [上线中间有预发布环境] i.e 只有测试人员通过特殊方法验证线上环境。
用户流量继续访问旧的环境。 `


## 2. 相关工作

DSP团队在面对一样的问题的时候， 提出了契约管理的方案。对服务的版本进行管理。服务注册的时候不替换复用旧的注册路径。 
`如此就不会出现[上线过程中, 服务不可用时期]`. 此时服务多版本同时存在。
问题没有解决的是， `1此时服务上线其实是有依赖的。 服务必须先上线才能够上线API Gateway, [上线不够简单]. 
另外，2. API Gateway由于目前只能替换上线. 不[支持预先发布环境].`  

1. 因为两个团队团队同时在处理这个问题。 还因为依赖的底层通信组件也不相同，所以造成了一些重复造轮子
2. 应该加强合作与讨论，各取所长， 在`7.未来工作` 阐述如何结合DSP方案的长处改进方案。 


## 3. 平滑上线

### 3.1 要解决的问题

#### 3.1.1. 服务化后上线部署复杂度增加.

JPaaS提供的灰度上线方案只支持单机上线。 不能够支持服务化.
以前只需要一个上线单进行上线。 现在要对每个受影响的服务进行上线， 并且在上线时需要考虑很多的点， 
包括:

1. 服务的依赖拓扑图， 需要根据依赖关系计算出上线顺序。 如果上游服务先升级， 可能会因为找不到下游的接口而不可用。 
2. 服务的严格的前向兼容， 服务API的不前向兼容把服务和调用方耦合在一起了。 如果服务不前向兼容， 

这个时候相当于依赖关系出现了环。 只能通过尽可能的同时对两个服务进行上线， 来减少服务不可用的时间。 

#### 3.1.2 上线后缺乏预发布环境。 

在以前， 单体上线的时候， 上线完四台机器的第一台机器后， 会通过设置host来访问这台新的机器来验证功能。 
测试和开发人员的功能验证是在实际用户访问之前的。 服务化之后， 由于负载均衡的机制， 没有了预发布验证的时间。 只能
等到和用户流量一起验证。 

### 3.2 问题拆解， 如何算解决了问题

1. 上线过程用户无感知， 有个预发布的时间区间供FE + RD 对服务进行验证
2. 上线的时候，各个服务不需要考虑依赖、先后顺序的上线， 当服务不前向兼容时，不会出现服务不可用状态
3. 上线自动化(一键上线和回滚)


### 3.3 如何解决

![服务架构](/image/20160524/architect.png)

大致的上线流程可以分为如下步骤

1. 线上有一个运行着的online1, 需要上线新的 online2
2. 用户流量继续访问 online1 进行访问旧的流量, QA & RD folks 的测试流量访问到 online2
3. 验证结束后， 将用户流量切换到 online2, 释放 online1 资源

这里面有两个关键技术难点要解决`1. 流量的标记和区分 2. 流量的切换`

1. 我们升级了底层组件Navi-rpc <sup>[7]</sup>的逻辑， 透传Context对象， 结合GO-BFE 支持通过Cookie 来区分流量来完成流量的标记
2. 我们利用GO-BFE的切换流量实时生效， 和Disconf推送配置实时生效来完成流量的切换。 

#### 3.3.1 依赖的基础框架和组件

- GO-BFE  <sup>[8]</sup> 可以根基cookie的特定key分流. 并且可以通过调用其提供的Open API 进行实时的修改流量分发规则
- Disconf  <sup>[5]</sup>  Disconf 能够集中化管理配置。 利用Zookeeper Watcher 回调的功能。 实现了部署的动态化
- JPaaS <sup>[9]</sup>  提供了上线的Open API, 包装这些API， 我实现了我们的上线程序 <sup>[10]</sup> 


#### 3.3.2 流量的标记和切换

**定义:**

- [边缘服务]指的是API Gateway 和前端静态资源。 这些接口没有通过Zookeeper 名称服务来路由。 是通过浏览器发起请求的BFE， BFE完成的路由。 
- [通用服务]边缘服务之外的服务。 这些服务通过Zookeeper 的名称服务来进行路由。 


#### I. 边缘服务的流量标记和切换
![边缘服务的流量标示和切换](/image/20160524/edge-service.png)

#### 流量的标记

-	通过BFE-WEB自助在BFE中配置规则。 如果用户设置static-ssp-version=v0 的cookie 并且请求前缀是 `/static`那么BFE 将流量转发至前端静态模块static-ssp-v0。 
-	如果用户设置ssp-web-version=v1 的cooki并且请求前缀是`/api` 那么BFE将流量转发至后端API Gateway ssp-web-v1. 
-	如此可以通过设置指定的版本来进行访问。 测试和开发人员通过设置cookie来访问预发布环境。 如此完成了边缘服务的流量标记。 


### 流量的切换

-	通过在BFE-WEB中配置默认规则， 即， 当用户流量(这些流量不会主动去设置cookie) 请求的时候， 前缀是`/static`, 那么BFE默认将流量发送到稳定的前端模块。 
-	当用户流量请求的前缀是`/api` 的时候， 那么BFE将流量转发至稳定的后端服务。 在测试和开发人员在预发布环境验证完成之后， 调用BFE的Open-API 修改稳定模块的指向。 
-	如此就完成了边缘服务流量切换。 


#### II. 通用服务的流量和标记
![通用服务流量的标记](/image/20160524/general-service-mark.png)

用户可以指定内部服务的访问版本。 比如说， 用户指定了delivery-version=v1 & adposition-version=v0. 这个时候，
API Gatway(webapi) 中将Cookie中的信息提取出来， 设置到RpcContext中， 该对象随着RPC调用向下游传递。 如此， delivery-v1服务在调用下游adposition 服务的时候， 可以根据RpcContext 中设置的adposition-version. 运行时决定调用的adposition的版本。 

#### III. 通用服务的流量的切换
![通用服务的流量的切换](/image/20160524/general-service-switch.png)

服务在注册的时候， 注册的是带版本的路径，比如 ｀/ssp/adposition/online/v0｀， 上游的调用方是复杂的。 甚至有可能是一个CT任务。 这个时候不能要求上游的服务跟着更改配置。 注意到， 上游的行为我们不能控制， 但是服务本身的行为我们可以控制。 服务和上游定的接口是Zookeeper 的注册路径。 

1. 我们让服务把`/ssp/adposition/online/v0` 的信息也注册到  `/ssp/adposition/online` 上就能让上游不感知到多版本的存在。  
2. 在开发和测试人员验证通过后，我们假设adposition-v0 是先前的稳定版本， adposition-v1 是要验证完成的版本。 在切换流量前， `/ssp/adposition/online/v0` 和 `/ssp/adposition/online` 保存的信息都是 adposition-v0 的服务的实例信息。 `/ssp/adposition/online/v1` 保存的事adposition-v1 的实例信息。 
3. 从Disconf处更改adposition 的正式版本 v0->v1。 因为配置实时生效。 adpsition-v0 实例发现配置里配置的不是v0, 就将自己的信息从 `/ssp/adposition/online`  中撤销。 adpsition-v1 实例发现配置里配置的不是v0, 就将自己的信息注册到 /ssp/adposition/online。 
4. 如此， 用户流量(不带服务版本信息的流量)能够从 `/ssp/adposition/online` 找到对应的服务实例。 <sup>[11]</sup> 阐述了具体的代码逻辑



#### 3.3.3 上线的自动化
![abt2 命令行工具](/image/20160524/abt2.png)

通过使用JPaaS & GO-BFE Open API， 包装了我们的上线程序。 屏蔽了内部结构的细节。 上线人员只需要将上线包输入程序， 再利用上线的命令即可完成上线工作。 
其中， 我设计了会话的概念。 使得我们在上线验证不通过的时候可以进行回滚。 
如上面的状态机图， 一次上线， 包含

```
0. 清空上次会话。 abt2 clean --op={op_name}
1. 开启会话 abt2 start create --op={op_name} --desp={description}
2. abt2 config put --op={op_name} --name={module_name} --path={war_path}
3. abt2 ship all --op={op_name}
4. abt2 status 获取到在浏览器控制台设置cookie访问预发布环境的的Cookie, 测试和开发人员利用这些Cookie在预发布环境进行验证
5. abt2 commit --op={op_name} 调用BFE API & Disconf API 进行流量的切换
6. 在会话没有结束之前， 都可以通过rollback 实施[切流量的回滚]
```

### 3.4 总结

在目前的实现方案下， 

1. 我们有了一个和用户流量分离的线上环境供验证。 
2. 上线不再需要考虑拓扑关系。 每次上线以会话为基本单位。不需要考虑服务间的依赖关系。 
3. 上线过程简单化。 和之前相比， 上线多个模块变成了批量的操作。 在上线过程中减少了不必要的人为操作 

但是

1. 以会话为最小单元对于跨产品的服务很不友好。 如果多个产品共享一个下游服务。 回滚操作不能支持部分回滚。 
2. 对于产品间短时的服务多版本需求不能支持。 而DSP方案就能够支持。


## 4. 其他相关工作

### 4.1 日志收集
我们服务化和容器技术的使用， 带来很多碎片的日志。 我们需要获取日志进行分析， 排查问题， 和统计工作。 
处于云端的服务不是固定的物理资源。 日志的获取难度增加了。 <sup>[4]</sup>  zhangxu 采用1. 在容器内部部署Flume收集数据
将数据发送到Kafka,  2. 集中收集数据并按照模块， 机器和日志等信息进行汇总和切割。 如此达到了实时集中
云端虚拟机器的日志的目的。 

### 4.2 监控和报警
和日志的问题类似， 由于服务的分布。 使得问题的排查更加困难。 <sup>[6]</sup> Prism采用java agent 的技术， 在极小的代码
侵入的情况下， 能够收集到大量定制的信息， 包含序错误， 统计接口访问时间， 甚至于业务信息userId等。

1. 其中程序的错误的展现能够展现整个调用链条的情况， 极大的方便了问题的排查。  
2. 访问时间的统计可以帮助统计接口性能，优化程序。
3. 通过收集userId 信息， 可以统计出活跃用户。 

棱镜信息的收集， 是一项伟大的工作。 很难想象服务化的过程中， 不能很好的监控。


### 4.3 配置管理
服务化带来的配置文件的增多。<sup>[5]</sup>  Disconf 能够集中化管理配置。 利用Zookeeper Watcher 回调的功能。 
实现了部署的动态化i.e. 更改配置， 无需重新打包和重启， 可实时生效。 我前面的平滑上线中的切流量就是通过
Disconf 实现的该功能做到在上线后控制程序的调用逻辑。 



## 5. 未来工作

## 5.1. 服务多版本

DSP的服务治理方案采用的多版本服务共存的方式， 能够支持多产品对同一服务不同版本更新的需求。 这类需求应当被支持。 
计划在API-Gateway 处增加一个调用方的版本控制。 也就是除了用户可以在浏览器上指定下游服务的版本。 各个产品也可以定制自己的Profile 定制属于自己的线上正式版本。 注意到此时服务多版本存在。 

### 5.2 性能的监控和准入测试

#### 1. 性能监控
相比于上线后， 通过日志分析发现性能的问题的方式。 replay 用户请求的方式更加主动。 提早发现问题。 减少用修复bug的方式来修复性能问题。 

#### 2. 准入测试
在预发环境中， 错开用户使用高峰期， 用用户真实数据replay Get请求对性能进行监控， 也能用真实数据发现一些潜在的问题.
这种方法和Dump数据库的方案相比有两个好处1. 线上库比较大， 环境准备工作需要另行开发。  2. 线上的环境更加真实。 像机器性能和外部依赖的UC等组件。 线上线下可能会有不同行为。 

### 5.3 持续发布

![GitFlow](/image/20160524/workflow.png)

结合工作流<sup>[12]</sup>的使用, 我们有办法做到持续发布。 QA Sign-Off 之后， RD将代码合进主干。 就进行自动的上线工作。 
注意到， 这个时候， QA在Develop 分支上进行测试。 Sign-Off 后， 代码合并进主干， 代码自动推送到线上。 测试和开发在预发布环境上做最后的验证。 验证通过后只需要切换流量即可完成上线。 


## 6. 总结

![开发效率](/image/20160524/productivity.png)


微服务实践<sup>[13]</sup>以来的一个体会是， 很多以前简单的事情变复杂了。 采用微服务一个比较公认重要的好处是， 因为模块相对小， 能够快速迭代。 然而在使用的过程中我认为无论是上线还是开发都变得复杂了， 如果没有相应的方法论的沉淀和工具的辅助， 子系统的迭代开发也做不到像想象中那么敏捷。 本文工作是对上线工作简化的一个尝试。 这种方法解决了SSP的问题, 但是实现方案不具有普适性。 


## 7. 参考资料

1. [microservices @Martin Fowler](http://martinfowler.com/articles/microservices.html)
2. [微服务架构浅谈 @PIPPO1980 Baidu ](http://cegtech.baidu.com/?p=819)
3. [实施微服务需要哪些基础框架 @杨波InfoQ](http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=400645575&idx=1&sn=da55d75db55117046c520de88dde1123&3rd=MzA3MDU4NTYzMw==&scene=6#rd)
4. [Flume+Kafka收集Docker容器内分布式日志应用实践 @zhangxu Baidu](http://neoremind.com/2016/05/flumekafka%E6%94%B6%E9%9B%86docker%E5%AE%B9%E5%99%A8%E5%86%85%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/)
5. [Distributed Configuration Management Platform(分布式配置管理平台) @liaoqiqi Baidu](https://github.com/knightliao/disconf)
6. [棱镜监控平台Prism @lixukun Baidu](http://hetu.baidu.com/api/platform/index?platformId=1749)
7. [分布式服务化框架Navi @zhangxu Baidu](http://wiki.baidu.com/pages/viewpage.action?pageId=36869734)
8. [GO-BFE @Team Baidu](http://wiki.baidu.com/pages/viewpage.action?pageId=142387788)
9. [JPaaS @Team Baidu](http://console-jpaas.baidu.com/hatch/index#/home)
10. [ABT2 SSP上线工具 @chenweidong Baidu](http://wiki.baidu.com/pages/viewpage.action?pageId=186650822)
11. [Navi-Rpc适应灰度发布的定制化修改 @chenweidong Baidu](http://wiki.baidu.com/pages/viewpage.action?pageId=181936021)
12. [工作流 @chenweidong Baidu](http://wiki.baidu.com/display/~chenweidong/Gitflow)
13. [MicroservicePremium @Martin Fowler](http://martinfowler.com/bliki/MicroservicePremium.html)